{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e24133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.9.1 in d:\\anaconda\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow-gpu==2.9.1 in d:\\anaconda\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: opencv-python in d:\\anaconda\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (14.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.47.0)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.20.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (3.2.1)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (21.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (3.10.0.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (0.26.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (2.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.1.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (0.4.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (3.19.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (2.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (2.9.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.12)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (1.1.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from tensorflow==2.9.1) (58.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\anaconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "#installing dependencies\n",
    "\n",
    "!pip install tensorflow==2.9.1 tensorflow-gpu==2.9.1 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01064c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76c2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D,Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8d111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid out of memory errors by setting GPU Memory Consumption Growth\n",
    "\n",
    "#accessing all the gpus on the machine\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\") \n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b42fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "733783ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the directories\n",
    "\n",
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wild dataset (http://vis-www.cs.umass.edu/lfw/)\n",
    "# uncompressing the Tar GZ Labelled Faces in the Wild Dataset\n",
    "\n",
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324baf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving LFW images to data/negative repository\n",
    "\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw',directory)):\n",
    "        EX_PATH = os.path.join('lfw',directory,file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bdc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f625413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# colleting positive and anchor classes\n",
    "\n",
    "#establishing a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # cutting down frame to 250x250\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    #collecting anchors\n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Creating unique file path \n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        \n",
    "    #collecting positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    #showing image back to the screen\n",
    "    cv2.imshow(\"Image Collection\", frame)\n",
    "    #breaking\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# releasing the webcam\n",
    "cap.release()\n",
    "# for closing the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07809b",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5404a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img):\n",
    "    data = []\n",
    "    for i in range(9):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        # img = tf.image.stateless_random_crop(img, size=(20,20,3), seed=(1,2))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            \n",
    "        data.append(img)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2577347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e10e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(ANC_PATH, '924e839c-135f-11ec-b54e-a0cec8d2d278.jpg')\n",
    "img = cv2.imread(img_path)\n",
    "augmented_images = data_aug(img)\n",
    "\n",
    "for image in augmented_images:\n",
    "    cv2.imwrite(os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(POS_PATH)):\n",
    "    img_path = os.path.join(POS_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    augmented_images = data_aug(img) \n",
    "    \n",
    "    for image in augmented_images:\n",
    "        cv2.imwrite(os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92100a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor, negative, positive created!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d866f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting image directories\n",
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'\\*.jpg').take(3000)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'\\*.jpg').take(3000)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*.jpg').take(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0908b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test = anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b185fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data\\\\anchor\\\\6db9cdd0-fb13-11ec-a668-002b67e194ec.jpg'\n"
     ]
    }
   ],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db654e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing -- scale and resize\n",
    "\n",
    "def preprocess(file_path):\n",
    "    # reading image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    #loading the image\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    # preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img,(100,100))\n",
    "    # scaling img b/w 0 and 1\n",
    "    img = img/255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "adef2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Labelled Datatset\n",
    "\n",
    "# (anchor, positive) => 1,1,1,1,1\n",
    "# (anchor, negative) => 0,0,0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26a9e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor,positive,tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor,negative,tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79366d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ebe5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6881a755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'data\\\\anchor\\\\6ca4c11c-fb13-11ec-9370-002b67e194ec.jpg',\n",
       " b'data\\\\positive\\\\8cc859a4-fb13-11ec-83e5-002b67e194ec.jpg',\n",
       " 1.0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98f31844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Train and Test Partition\n",
    "\n",
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img),preprocess(validation_img),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46747a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[0.8235294 , 0.79607844, 0.7647059 ],\n",
       "         [0.8245098 , 0.7970588 , 0.7656863 ],\n",
       "         [0.8333333 , 0.8       , 0.77156866],\n",
       "         ...,\n",
       "         [0.90563726, 0.8860294 , 0.8625    ],\n",
       "         [0.90416664, 0.8845588 , 0.8610294 ],\n",
       "         [0.904902  , 0.88529414, 0.8617647 ]],\n",
       " \n",
       "        [[0.82843137, 0.79901963, 0.7705882 ],\n",
       "         [0.83431375, 0.80490196, 0.7764706 ],\n",
       "         [0.8394608 , 0.80563724, 0.7794118 ],\n",
       "         ...,\n",
       "         [0.90906864, 0.8894608 , 0.8659314 ],\n",
       "         [0.90612745, 0.8865196 , 0.8629902 ],\n",
       "         [0.9019608 , 0.88235295, 0.85882354]],\n",
       " \n",
       "        [[0.8352941 , 0.8       , 0.7745098 ],\n",
       "         [0.8401961 , 0.80490196, 0.7794118 ],\n",
       "         [0.84607846, 0.81078434, 0.7852941 ],\n",
       "         ...,\n",
       "         [0.91495097, 0.8953431 , 0.8718137 ],\n",
       "         [0.9127451 , 0.8931373 , 0.86960787],\n",
       "         [0.9127451 , 0.8931373 , 0.86960787]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.9078431 , 0.9       , 0.85294116],\n",
       "         [0.92058825, 0.9127451 , 0.8656863 ],\n",
       "         [0.9137255 , 0.90588236, 0.85882354],\n",
       "         ...,\n",
       "         [0.8392157 , 0.8571078 , 0.9482843 ],\n",
       "         [0.8490196 , 0.86151963, 0.95465684],\n",
       "         [0.8455882 , 0.85808825, 0.9512255 ]],\n",
       " \n",
       "        [[0.92034316, 0.9125    , 0.8654412 ],\n",
       "         [0.92156863, 0.9137255 , 0.8666667 ],\n",
       "         [0.9169118 , 0.90686274, 0.8664216 ],\n",
       "         ...,\n",
       "         [0.8066176 , 0.83210784, 0.9370098 ],\n",
       "         [0.8247549 , 0.8502451 , 0.9551471 ],\n",
       "         [0.82965684, 0.85514706, 0.96004903]],\n",
       " \n",
       "        [[0.92058825, 0.9127451 , 0.8617647 ],\n",
       "         [0.9196078 , 0.9117647 , 0.86470586],\n",
       "         [0.9137255 , 0.90294117, 0.86470586],\n",
       "         ...,\n",
       "         [0.8093137 , 0.84068626, 0.93088233],\n",
       "         [0.8377451 , 0.8691176 , 0.95931375],\n",
       "         [0.8664216 , 0.8977941 , 0.9879902 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(100, 100, 3), dtype=float32, numpy=\n",
       " array([[[0.9963235 , 0.9730392 , 0.9730392 ],\n",
       "         [0.9848039 , 0.9632353 , 0.9622549 ],\n",
       "         [0.9698529 , 0.95416665, 0.9502451 ],\n",
       "         ...,\n",
       "         [0.77205884, 0.76740193, 0.7882353 ],\n",
       "         [1.        , 0.9990196 , 0.9941176 ],\n",
       "         [0.9973039 , 0.9965686 , 0.9740196 ]],\n",
       " \n",
       "        [[0.9872549 , 0.97156864, 0.96568626],\n",
       "         [0.9852941 , 0.96960783, 0.9637255 ],\n",
       "         [0.9617647 , 0.9460784 , 0.93578434],\n",
       "         ...,\n",
       "         [0.3112745 , 0.2987745 , 0.34264705],\n",
       "         [0.91789216, 0.90906864, 0.9230392 ],\n",
       "         [0.9889706 , 0.9821078 , 0.96813726]],\n",
       " \n",
       "        [[0.99191177, 0.9762255 , 0.9644608 ],\n",
       "         [0.98014706, 0.9644608 , 0.9526961 ],\n",
       "         [0.9627451 , 0.9492647 , 0.93308824],\n",
       "         ...,\n",
       "         [0.2134804 , 0.18970588, 0.2497549 ],\n",
       "         [0.3389706 , 0.31691176, 0.35514706],\n",
       "         [0.9411765 , 0.92401963, 0.9340686 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.8301471 , 0.8066176 , 0.75171566],\n",
       "         [0.8379902 , 0.8144608 , 0.7595588 ],\n",
       "         [0.84705883, 0.8235294 , 0.76862746],\n",
       "         ...,\n",
       "         [0.44877452, 0.41642156, 0.42328432],\n",
       "         [0.44117647, 0.42965686, 0.4502451 ],\n",
       "         [0.39583334, 0.39240196, 0.4127451 ]],\n",
       " \n",
       "        [[0.8235294 , 0.8       , 0.74509805],\n",
       "         [0.82843137, 0.80490196, 0.75      ],\n",
       "         [0.8375    , 0.81397057, 0.7590686 ],\n",
       "         ...,\n",
       "         [0.4404412 , 0.41789216, 0.42965686],\n",
       "         [0.41813725, 0.4125    , 0.4365196 ],\n",
       "         [0.3632353 , 0.36862746, 0.3882353 ]],\n",
       " \n",
       "        [[0.8335784 , 0.80220586, 0.7590686 ],\n",
       "         [0.8392157 , 0.80784315, 0.7647059 ],\n",
       "         [0.83431375, 0.8029412 , 0.75980395],\n",
       "         ...,\n",
       "         [0.4120098 , 0.39019608, 0.40808824],\n",
       "         [0.42818627, 0.4242647 , 0.45367646],\n",
       "         [0.37254903, 0.38627452, 0.4127451 ]]], dtype=float32)>,\n",
       " 1.0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = preprocess_twin(*examples)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc711f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building dataloader pipeline\n",
    "\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0590801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Partition\n",
    "\n",
    "train_data = data.take(round(len(data)*0.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3d3440a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = train_data.as_numpy_iterator()\n",
    "train_sample = train_samples.next()\n",
    "len(train_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b87abb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Partition\n",
    "\n",
    "test_data = data.skip(round(len(data)*0.7))\n",
    "test_data = test_data.take(round(len(data)*0.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a108db",
   "metadata": {},
   "source": [
    "## MODEL ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3a362b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(100,100,3), name='input_image')\n",
    "    \n",
    "#First Block\n",
    "c1 = Conv2D(64,(10,10), activation='relu')(inp)\n",
    "m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "# Second block\n",
    "c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "# Third block \n",
    "c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "# Final embedding block\n",
    "c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "f1 = Flatten()(c4)\n",
    "d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "mod = Model(inputs=[inp],outputs=[d1] ,name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fad04250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 46, 46, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 40, 40, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 20, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 17, 17, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "beef9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building embedding layer  (according to paper (pg.4))\n",
    "\n",
    "def make_embedding():\n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "    \n",
    "    #First Block\n",
    "    c1 = Conv2D(64,(10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # Third block \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    return Model(inputs=[inp],outputs=[d1] ,name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "37003986",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6396241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 46, 46, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 40, 40, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 20, 20, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 17, 17, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "064b033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Distance Layer\n",
    "\n",
    "\n",
    "#creating L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    # Inheritance\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    # similarity calculation\n",
    "    def cell(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bda54d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Siamese Model\n",
    "# siamese neural network\n",
    "def make_siamese_model(): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "    \n",
    "    # Combining siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "925a502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "728640b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
      "                                                                  'validation_img[0][0]']         \n",
      "                                                                                                  \n",
      " distance (L1Dist)              (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            4097        ['distance[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6413fbb",
   "metadata": {},
   "source": [
    "## TRAINING OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d53e2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP LOSS AND OPTIMIZER\n",
    "# tf.losses.BinaryCrossentropy??\n",
    "\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "677e3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "789949ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTABLISHING CHECKPOINTS\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1aa4e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING TRAIN STEP FUNCTION\n",
    "\n",
    "#The basic flow for training of one batch:\n",
    "#    1.Make a prediction\n",
    "#    2.Calculate loss\n",
    "#   3.Derive gradients\n",
    "#   4.Calculate new weights and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7151dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = train_data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "85de5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = test_batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9520b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = batch_1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e44ae6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 100, 100, 3)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f1b1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = batch_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a1fb125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d85fab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Recording all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        # Getting anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Getting our label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculating loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    print(loss)\n",
    "        \n",
    "    # Calculating gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculating updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "231acbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE TRAINING LOOP\n",
    "\n",
    "def train(data, EPOCHS):\n",
    "    # Looping through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Looping through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Running train step \n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        # Saving checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2861df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/50\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "20/21 [===========================>..] - ETA: 12sTensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "21/21 [==============================] - 256s 12s/step\n",
      "\n",
      " Epoch 2/50\n",
      "21/21 [==============================] - 260s 12s/step\n",
      "\n",
      " Epoch 3/50\n",
      "21/21 [==============================] - 256s 12s/step\n",
      "\n",
      " Epoch 4/50\n",
      "21/21 [==============================] - 253s 12s/step\n",
      "\n",
      " Epoch 5/50\n",
      "21/21 [==============================] - 251s 12s/step\n",
      "\n",
      " Epoch 6/50\n",
      "21/21 [==============================] - 257s 12s/step\n",
      "\n",
      " Epoch 7/50\n",
      "21/21 [==============================] - 255s 12s/step\n",
      "\n",
      " Epoch 8/50\n",
      "21/21 [==============================] - 256s 12s/step\n",
      "\n",
      " Epoch 9/50\n",
      "21/21 [==============================] - 261s 12s/step\n",
      "\n",
      " Epoch 10/50\n",
      "21/21 [==============================] - 256s 12s/step\n",
      "\n",
      " Epoch 11/50\n",
      "21/21 [==============================] - 260s 12s/step\n",
      "\n",
      " Epoch 12/50\n",
      "21/21 [==============================] - 259s 12s/step\n",
      "\n",
      " Epoch 13/50\n",
      "21/21 [==============================] - 262s 12s/step\n",
      "\n",
      " Epoch 14/50\n",
      "21/21 [==============================] - 262s 12s/step\n",
      "\n",
      " Epoch 15/50\n",
      "21/21 [==============================] - 262s 12s/step\n",
      "\n",
      " Epoch 16/50\n",
      "21/21 [==============================] - 261s 12s/step\n",
      "\n",
      " Epoch 17/50\n",
      "21/21 [==============================] - 265s 13s/step\n",
      "\n",
      " Epoch 18/50\n",
      "21/21 [==============================] - 269s 13s/step\n",
      "\n",
      " Epoch 19/50\n",
      "21/21 [==============================] - 271s 13s/step\n",
      "\n",
      " Epoch 20/50\n",
      "21/21 [==============================] - 268s 13s/step\n",
      "\n",
      " Epoch 21/50\n",
      "21/21 [==============================] - 269s 13s/step\n",
      "\n",
      " Epoch 22/50\n",
      "21/21 [==============================] - 265s 13s/step\n",
      "\n",
      " Epoch 23/50\n",
      "21/21 [==============================] - 267s 13s/step\n",
      "\n",
      " Epoch 24/50\n",
      "21/21 [==============================] - 268s 13s/step\n",
      "\n",
      " Epoch 25/50\n",
      "21/21 [==============================] - 273s 13s/step\n",
      "\n",
      " Epoch 26/50\n",
      "21/21 [==============================] - 260s 12s/step\n",
      "\n",
      " Epoch 27/50\n",
      "21/21 [==============================] - 259s 12s/step\n",
      "\n",
      " Epoch 28/50\n",
      "21/21 [==============================] - 260s 12s/step\n",
      "\n",
      " Epoch 29/50\n",
      "21/21 [==============================] - 259s 12s/step\n",
      "\n",
      " Epoch 30/50\n",
      "21/21 [==============================] - 264s 13s/step\n",
      "\n",
      " Epoch 31/50\n",
      "21/21 [==============================] - 268s 13s/step\n",
      "\n",
      " Epoch 32/50\n",
      "21/21 [==============================] - 271s 13s/step\n",
      "\n",
      " Epoch 33/50\n",
      "21/21 [==============================] - 276s 13s/step\n",
      "\n",
      " Epoch 34/50\n",
      "21/21 [==============================] - 276s 13s/step\n",
      "\n",
      " Epoch 35/50\n",
      "21/21 [==============================] - 276s 13s/step\n",
      "\n",
      " Epoch 36/50\n",
      "21/21 [==============================] - 276s 13s/step\n",
      "\n",
      " Epoch 37/50\n",
      "21/21 [==============================] - 278s 13s/step\n",
      "\n",
      " Epoch 38/50\n",
      "21/21 [==============================] - 278s 13s/step\n",
      "\n",
      " Epoch 39/50\n",
      "21/21 [==============================] - 278s 13s/step\n",
      "\n",
      " Epoch 40/50\n",
      "21/21 [==============================] - 280s 13s/step\n",
      "\n",
      " Epoch 41/50\n",
      "21/21 [==============================] - 280s 13s/step\n",
      "\n",
      " Epoch 42/50\n",
      "21/21 [==============================] - 280s 13s/step\n",
      "\n",
      " Epoch 43/50\n",
      "21/21 [==============================] - 281s 13s/step\n",
      "\n",
      " Epoch 44/50\n",
      "21/21 [==============================] - 280s 13s/step\n",
      "\n",
      " Epoch 45/50\n",
      "21/21 [==============================] - 279s 13s/step\n",
      "\n",
      " Epoch 46/50\n",
      "21/21 [==============================] - 283s 13s/step\n",
      "\n",
      " Epoch 47/50\n",
      "21/21 [==============================] - 281s 13s/step\n",
      "\n",
      " Epoch 48/50\n",
      "21/21 [==============================] - 281s 13s/step\n",
      "\n",
      " Epoch 49/50\n",
      "21/21 [==============================] - 280s 13s/step\n",
      "\n",
      " Epoch 50/50\n",
      "21/21 [==============================] - 283s 13s/step\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL\n",
    "\n",
    "EPOCHS = 50\n",
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074a3f3",
   "metadata": {},
   "source": [
    "## EVALUATING OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ae30f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing metric calculations\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3c113557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a batch of data\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b06338ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_var = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "877539c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0a830c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 921ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions\n",
    "y_hat = siamese_model.predict([test_input, test_val])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a4abd70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post processing the results \n",
    "[1 if prediction > 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6ece5453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8181cd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CALCULATING METRICS\n",
    "\n",
    "# Creating a metric object \n",
    "m = Recall()\n",
    "\n",
    "# Calculating the recall value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Returning Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cc401146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a metric object \n",
    "m = Precision()\n",
    "\n",
    "# Calculating the precision value \n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Returning precision Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "    yhat = siamese_model.predict([test_input, test_val])\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true, yhat)\n",
    "\n",
    "print(r.result().numpy(),p.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6bde7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # RESULT VISUALIZATION\n",
    "\n",
    "# Setting plot size \n",
    "    #plt.figure(figsize=(10,8))\n",
    "\n",
    "# Setting first subplot\n",
    "    #plt.subplot(1,2,1)\n",
    "    #plt.imshow(test_input[0])\n",
    "\n",
    "# Setting second subplot\n",
    "    #plt.subplot(1,2,2)\n",
    "    #plt.imshow(test_val[0])\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79893ea3",
   "metadata": {},
   "source": [
    "## SAVING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "deeda572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Saving weights\n",
    "siamese_model.save('siamesemodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bedacf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.L1Dist"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ff8f513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Reloading our model \n",
    "model = tf.keras.models.load_model('siamesemodel.h5', \n",
    "                                   custom_objects={'L1Dist':L1Dist, \n",
    "                                                   'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "031897eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 552ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making predictions with reloaded model\n",
    "model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5930669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]']              \n",
      "                                                                                                  \n",
      " l1_dist_2 (L1Dist)             (None, 4096)         0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            4097        ['l1_dist_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff7c4e",
   "metadata": {},
   "source": [
    "## REAL TIME TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e78e4c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7ccf7617-fb13-11ec-81a2-002b67e194ec.jpg',\n",
       " '7dcb1f30-fb13-11ec-844b-002b67e194ec.jpg',\n",
       " '7e8fba3c-fb13-11ec-ad66-002b67e194ec.jpg',\n",
       " '7f3faeb7-fb13-11ec-a0f9-002b67e194ec.jpg',\n",
       " '82e861d4-fb13-11ec-b532-002b67e194ec.jpg',\n",
       " '83df40fb-fb13-11ec-ad64-002b67e194ec.jpg',\n",
       " '85269ad9-fb13-11ec-b199-002b67e194ec.jpg',\n",
       " '861d6295-fb13-11ec-89d1-002b67e194ec.jpg',\n",
       " '864a79d4-fb13-11ec-9017-002b67e194ec.jpg',\n",
       " '8722efb6-fb13-11ec-b80d-002b67e194ec.jpg',\n",
       " '87904331-fb13-11ec-88c6-002b67e194ec.jpg',\n",
       " '87c44817-fb13-11ec-b990-002b67e194ec.jpg',\n",
       " '87f1ae1e-fb13-11ec-9e21-002b67e194ec.jpg',\n",
       " '88f749cd-fb13-11ec-b8ae-002b67e194ec.jpg',\n",
       " '891f8720-fb13-11ec-9392-002b67e194ec.jpg',\n",
       " '89c60237-fb13-11ec-9eb5-002b67e194ec.jpg',\n",
       " '8abca5bd-fb13-11ec-92b3-002b67e194ec.jpg',\n",
       " '8ae4eb25-fb13-11ec-8046-002b67e194ec.jpg',\n",
       " '8b6ce4ef-fb13-11ec-8a90-002b67e194ec.jpg',\n",
       " '8bf9f016-fb13-11ec-9e2c-002b67e194ec.jpg',\n",
       " '92a657d6-fb13-11ec-9f4a-002b67e194ec.jpg',\n",
       " '92f69e15-fb13-11ec-88a3-002b67e194ec.jpg',\n",
       " '934ccb4d-fb13-11ec-8782-002b67e194ec.jpg',\n",
       " '93748cfe-fb13-11ec-9fbc-002b67e194ec.jpg',\n",
       " '949db26e-fb13-11ec-8866-002b67e194ec.jpg',\n",
       " '94d027ff-fb13-11ec-8b5c-002b67e194ec.jpg',\n",
       " 'b2344ff6-fb13-11ec-970c-002b67e194ec.jpg',\n",
       " 'b253755c-fb13-11ec-a918-002b67e194ec.jpg',\n",
       " 'b28a1993-fb13-11ec-8eb6-002b67e194ec.jpg',\n",
       " 'b3039f30-fb13-11ec-a4ea-002b67e194ec.jpg',\n",
       " 'b45506b4-fb13-11ec-a571-002b67e194ec.jpg',\n",
       " 'b4fb4df5-fb13-11ec-b758-002b67e194ec.jpg',\n",
       " 'b7b1fb27-fb13-11ec-bf6d-002b67e194ec.jpg',\n",
       " 'b80cef78-fb13-11ec-ad95-002b67e194ec.jpg',\n",
       " 'ba1359eb-fb13-11ec-a601-002b67e194ec.jpg',\n",
       " 'bb194596-fb13-11ec-886c-002b67e194ec.jpg',\n",
       " 'bb73dcb3-fb13-11ec-8fff-002b67e194ec.jpg',\n",
       " 'bc51a28c-fb13-11ec-8505-002b67e194ec.jpg',\n",
       " 'bced8628-fb13-11ec-baad-002b67e194ec.jpg',\n",
       " 'bdc661bb-fb13-11ec-8a63-002b67e194ec.jpg',\n",
       " 'be531f35-fb13-11ec-8b09-002b67e194ec.jpg',\n",
       " 'bed0b213-fb13-11ec-9841-002b67e194ec.jpg',\n",
       " 'bf53ecdb-fb13-11ec-8c32-002b67e194ec.jpg',\n",
       " 'bfa486af-fb13-11ec-a991-002b67e194ec.jpg',\n",
       " 'c1fb83f2-fb13-11ec-b422-002b67e194ec.jpg',\n",
       " 'c31ab5fc-fb13-11ec-a9ce-002b67e194ec.jpg',\n",
       " 'c3ba0f25-fb13-11ec-aac4-002b67e194ec.jpg',\n",
       " 'c4166769-fb13-11ec-8a6d-002b67e194ec.jpg',\n",
       " 'c56cd3ee-fb13-11ec-b507-002b67e194ec.jpg',\n",
       " 'c5b6d051-fb13-11ec-bd78-002b67e194ec.jpg']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join('application_data', 'verification_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "448e65a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application_data\\verification_images\\7ccf7617-fb13-11ec-81a2-002b67e194ec.jpg\n",
      "application_data\\verification_images\\7dcb1f30-fb13-11ec-844b-002b67e194ec.jpg\n",
      "application_data\\verification_images\\7e8fba3c-fb13-11ec-ad66-002b67e194ec.jpg\n",
      "application_data\\verification_images\\7f3faeb7-fb13-11ec-a0f9-002b67e194ec.jpg\n",
      "application_data\\verification_images\\82e861d4-fb13-11ec-b532-002b67e194ec.jpg\n",
      "application_data\\verification_images\\83df40fb-fb13-11ec-ad64-002b67e194ec.jpg\n",
      "application_data\\verification_images\\85269ad9-fb13-11ec-b199-002b67e194ec.jpg\n",
      "application_data\\verification_images\\861d6295-fb13-11ec-89d1-002b67e194ec.jpg\n",
      "application_data\\verification_images\\864a79d4-fb13-11ec-9017-002b67e194ec.jpg\n",
      "application_data\\verification_images\\8722efb6-fb13-11ec-b80d-002b67e194ec.jpg\n",
      "application_data\\verification_images\\87904331-fb13-11ec-88c6-002b67e194ec.jpg\n",
      "application_data\\verification_images\\87c44817-fb13-11ec-b990-002b67e194ec.jpg\n",
      "application_data\\verification_images\\87f1ae1e-fb13-11ec-9e21-002b67e194ec.jpg\n",
      "application_data\\verification_images\\88f749cd-fb13-11ec-b8ae-002b67e194ec.jpg\n",
      "application_data\\verification_images\\891f8720-fb13-11ec-9392-002b67e194ec.jpg\n",
      "application_data\\verification_images\\89c60237-fb13-11ec-9eb5-002b67e194ec.jpg\n",
      "application_data\\verification_images\\8abca5bd-fb13-11ec-92b3-002b67e194ec.jpg\n",
      "application_data\\verification_images\\8ae4eb25-fb13-11ec-8046-002b67e194ec.jpg\n",
      "application_data\\verification_images\\8b6ce4ef-fb13-11ec-8a90-002b67e194ec.jpg\n",
      "application_data\\verification_images\\8bf9f016-fb13-11ec-9e2c-002b67e194ec.jpg\n",
      "application_data\\verification_images\\92a657d6-fb13-11ec-9f4a-002b67e194ec.jpg\n",
      "application_data\\verification_images\\92f69e15-fb13-11ec-88a3-002b67e194ec.jpg\n",
      "application_data\\verification_images\\934ccb4d-fb13-11ec-8782-002b67e194ec.jpg\n",
      "application_data\\verification_images\\93748cfe-fb13-11ec-9fbc-002b67e194ec.jpg\n",
      "application_data\\verification_images\\949db26e-fb13-11ec-8866-002b67e194ec.jpg\n",
      "application_data\\verification_images\\94d027ff-fb13-11ec-8b5c-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b2344ff6-fb13-11ec-970c-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b253755c-fb13-11ec-a918-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b28a1993-fb13-11ec-8eb6-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b3039f30-fb13-11ec-a4ea-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b45506b4-fb13-11ec-a571-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b4fb4df5-fb13-11ec-b758-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b7b1fb27-fb13-11ec-bf6d-002b67e194ec.jpg\n",
      "application_data\\verification_images\\b80cef78-fb13-11ec-ad95-002b67e194ec.jpg\n",
      "application_data\\verification_images\\ba1359eb-fb13-11ec-a601-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bb194596-fb13-11ec-886c-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bb73dcb3-fb13-11ec-8fff-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bc51a28c-fb13-11ec-8505-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bced8628-fb13-11ec-baad-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bdc661bb-fb13-11ec-8a63-002b67e194ec.jpg\n",
      "application_data\\verification_images\\be531f35-fb13-11ec-8b09-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bed0b213-fb13-11ec-9841-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bf53ecdb-fb13-11ec-8c32-002b67e194ec.jpg\n",
      "application_data\\verification_images\\bfa486af-fb13-11ec-a991-002b67e194ec.jpg\n",
      "application_data\\verification_images\\c1fb83f2-fb13-11ec-b422-002b67e194ec.jpg\n",
      "application_data\\verification_images\\c31ab5fc-fb13-11ec-a9ce-002b67e194ec.jpg\n",
      "application_data\\verification_images\\c3ba0f25-fb13-11ec-aac4-002b67e194ec.jpg\n",
      "application_data\\verification_images\\c4166769-fb13-11ec-8a6d-002b67e194ec.jpg\n",
      "application_data\\verification_images\\c56cd3ee-fb13-11ec-b507-002b67e194ec.jpg\n",
      "application_data\\verification_images\\c5b6d051-fb13-11ec-bd78-002b67e194ec.jpg\n"
     ]
    }
   ],
   "source": [
    "for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "    validation_img = os.path.join('application_data', 'verification_images', image)\n",
    "    print(validation_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f5cc0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification Function\n",
    "\n",
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Building results array\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data', 'verification_images')):\n",
    "        input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg'))\n",
    "        validation_img = preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "        \n",
    "        # Making Predictions \n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        results.append(result)\n",
    "    \n",
    "    # Detection Threshold: Metric above which a prediciton is considered positive \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images'))) \n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1e7d2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV REAL TIME VERIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0eaf56e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 525ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "True\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    cv2.imshow('Verification', frame)\n",
    "    \n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # Save input image to application_data/input_image folder \n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "        # Running for verification\n",
    "        results, verified = verify(model, 0.9, 0.7)\n",
    "        print(verified)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "80241d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.squeeze(results) > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0c4ead72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE END "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
